{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Building Complex Agent Workflows with LangGraph\n",
        "\n",
        "This notebook demonstrates how to build complex agent workflows using LangGraph, a framework for creating stateful, multi-agent systems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install -U langgraph langchain_community langchain_anthropic langchain_experimental"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "\n",
        "def _set_if_undefined(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"Please provide your {var}\")\n",
        "\n",
        "\n",
        "_set_if_undefined(\"ANTHROPIC_API_KEY\")\n",
        "_set_if_undefined(\"TAVILY_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"admonition tip\">\n",
        "    <p class=\"admonition-title\">Set up <a href=\"https://smith.langchain.com\">LangSmith</a> for LangGraph development</p>\n",
        "    <p style=\"padding-top: 5px;\">\n",
        "        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph â€” read more about how to get started <a href=\"https://docs.smith.langchain.com\">here</a>. \n",
        "    </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create tools\n",
        "\n",
        "For this example, you will make an agent to do web research with a search engine, and one agent to create plots. Define the tools they'll use below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Annotated\n",
        "\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_core.tools import tool\n",
        "from langchain_experimental.utilities import PythonREPL\n",
        "\n",
        "tavily_tool = TavilySearchResults(max_results=5)\n",
        "\n",
        "# This executes code locally, which can be unsafe\n",
        "repl = PythonREPL()\n",
        "\n",
        "\n",
        "@tool\n",
        "def python_repl_tool(\n",
        "    code: Annotated[str, \"The python code to execute to generate your chart.\"],\n",
        "):\n",
        "    \"\"\"Use this to execute python code and do math. If you want to see the output of a value,\n",
        "    you should print it out with `print(...)`. This is visible to the user.\"\"\"\n",
        "    try:\n",
        "        result = repl.run(code)\n",
        "    except BaseException as e:\n",
        "        return f\"Failed to execute. Error: {repr(e)}\"\n",
        "    result_str = f\"Successfully executed:\\n```python\\n{code}\\n```\\nStdout: {result}\"\n",
        "    return result_str"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Agent Supervisor\n",
        "\n",
        "It will use LLM with structured output to choose the next worker node OR finish processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Literal\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langgraph.graph import MessagesState, END\n",
        "from langgraph.types import Command\n",
        "\n",
        "\n",
        "members = [\"researcher\", \"coder\"]\n",
        "# Our team supervisor is an LLM node. It just picks the next agent to process\n",
        "# and decides when the work is completed\n",
        "options = members + [\"FINISH\"]\n",
        "\n",
        "system_prompt = (\n",
        "    \"You are a supervisor tasked with managing a conversation between the\"\n",
        "    f\" following workers: {members}. Given the following user request,\"\n",
        "    \" respond with the worker to act next. Each worker will perform a\"\n",
        "    \" task and respond with their results and status. When finished,\"\n",
        "    \" respond with FINISH.\"\n",
        ")\n",
        "\n",
        "\n",
        "class Router(TypedDict):\n",
        "    \"\"\"Worker to route to next. If no workers needed, route to FINISH.\"\"\"\n",
        "\n",
        "    next: Literal[*options]\n",
        "\n",
        "\n",
        "llm = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\n",
        "\n",
        "\n",
        "class State(MessagesState):\n",
        "    next: str\n",
        "\n",
        "\n",
        "def supervisor_node(state: State) -> Command[Literal[*members, \"__end__\"]]:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "    ] + state[\"messages\"]\n",
        "    response = llm.with_structured_output(Router).invoke(messages)\n",
        "    goto = response[\"next\"]\n",
        "    if goto == \"FINISH\":\n",
        "        goto = END\n",
        "\n",
        "    return Command(goto=goto, update={\"next\": goto})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Construct Graph\n",
        "\n",
        "We're ready to start building the graph. Below, define the state and worker nodes using the function we just defined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "\n",
        "research_agent = create_react_agent(\n",
        "    llm, tools=[tavily_tool], prompt=\"You are a researcher. DO NOT do any math.\"\n",
        ")\n",
        "\n",
        "\n",
        "def research_node(state: State) -> Command[Literal[\"supervisor\"]]:\n",
        "    result = research_agent.invoke(state)\n",
        "    return Command(\n",
        "        update={\n",
        "            \"messages\": [\n",
        "                HumanMessage(content=result[\"messages\"][-1].content, name=\"researcher\")\n",
        "            ]\n",
        "        },\n",
        "        goto=\"supervisor\",\n",
        "    )\n",
        "\n",
        "\n",
        "# NOTE: THIS PERFORMS ARBITRARY CODE EXECUTION, WHICH CAN BE UNSAFE WHEN NOT SANDBOXED\n",
        "code_agent = create_react_agent(llm, tools=[python_repl_tool])\n",
        "\n",
        "\n",
        "def code_node(state: State) -> Command[Literal[\"supervisor\"]]:\n",
        "    result = code_agent.invoke(state)\n",
        "    return Command(\n",
        "        update={\n",
        "            \"messages\": [\n",
        "                HumanMessage(content=result[\"messages\"][-1].content, name=\"coder\")\n",
        "            ]\n",
        "        },\n",
        "        goto=\"supervisor\",\n",
        "    )\n",
        "\n",
        "\n",
        "builder = StateGraph(State)\n",
        "builder.add_edge(START, \"supervisor\")\n",
        "builder.add_node(\"supervisor\", supervisor_node)\n",
        "builder.add_node(\"researcher\", research_node)\n",
        "builder.add_node(\"coder\", code_node)\n",
        "graph = builder.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the graph with a query\n",
        "query = \"What is the average net worth of the top 5 richest people in the world? Create a bar chart.\"\n",
        "\n",
        "# Create an event stream\n",
        "events = graph.stream({\"messages\": [{\"role\": \"user\", \"content\": query}]})\n",
        "\n",
        "# Process each event\n",
        "for event in events:\n",
        "    node = event.get(\"node\")\n",
        "    if node is not None:\n",
        "        print(f\"\\n{'-' * 50}\")\n",
        "        print(f\"Currently executing node: {node}\")\n",
        "        if node == \"supervisor\":\n",
        "            print(f\"Next: {event['state'].get('next', 'Not specified')}\")\n",
        "    \n",
        "    state = event.get(\"state\")\n",
        "    if state is not None and \"messages\" in state:\n",
        "        # Print the last message\n",
        "        last_message = state[\"messages\"][-1]\n",
        "        if hasattr(last_message, \"name\") and last_message.name:\n",
        "            print(f\"\\n{last_message.name.upper()}: {last_message.content[:200]}...\")\n",
        "        else:\n",
        "            print(f\"\\n{last_message['role'].upper()}: {last_message['content'][:200]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building an Enhanced Multi-Agent System\n",
        "\n",
        "Now let's extend this concept to create a more complex system with additional agents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "# Define additional specialized tools\n",
        "@tool\n",
        "def summarize_text(text: Annotated[str, \"The text to summarize\"]):\n",
        "    \"\"\"Summarize the provided text.\"\"\"\n",
        "    # In a real implementation, this would use an LLM or specialized summarization model\n",
        "    return f\"Summary of {len(text)} characters of text: This is a placeholder summary.\"\n",
        "\n",
        "@tool\n",
        "def analyze_sentiment(text: Annotated[str, \"The text to analyze for sentiment\"]):\n",
        "    \"\"\"Analyze the sentiment of the provided text.\"\"\"\n",
        "    # Mock implementation\n",
        "    if \"good\" in text.lower() or \"great\" in text.lower() or \"excellent\" in text.lower():\n",
        "        return \"Positive sentiment detected\"\n",
        "    elif \"bad\" in text.lower() or \"terrible\" in text.lower() or \"awful\" in text.lower():\n",
        "        return \"Negative sentiment detected\"\n",
        "    else:\n",
        "        return \"Neutral sentiment detected\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a more complex routing system with additional agents\n",
        "enhanced_members = [\"researcher\", \"coder\", \"summarizer\", \"analyst\"]\n",
        "enhanced_options = enhanced_members + [\"FINISH\"]\n",
        "\n",
        "enhanced_system_prompt = (\n",
        "    \"You are a supervisor tasked with managing a conversation between the\"\n",
        "    f\" following workers: {enhanced_members}. Given the following user request,\"\n",
        "    \" respond with the worker to act next. Each worker will perform a\"\n",
        "    \" task and respond with their results and status.\\n\\n\"\n",
        "    \"Researcher: Finds information on topics using search.\\n\"\n",
        "    \"Coder: Writes and executes Python code, creates visualizations.\\n\"\n",
        "    \"Summarizer: Creates concise summaries of text.\\n\"\n",
        "    \"Analyst: Analyzes sentiment and provides insights.\\n\\n\"\n",
        "    \"When no further work is needed, respond with FINISH.\"\n",
        ")\n",
        "\n",
        "\n",
        "class EnhancedRouter(TypedDict):\n",
        "    \"\"\"Worker to route to next. If no workers needed, route to FINISH.\"\"\"\n",
        "\n",
        "    next: Literal[*enhanced_options]\n",
        "\n",
        "\n",
        "def enhanced_supervisor_node(state: State) -> Command[Literal[*enhanced_members, \"__end__\"]]:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": enhanced_system_prompt},\n",
        "    ] + state[\"messages\"]\n",
        "    response = llm.with_structured_output(EnhancedRouter).invoke(messages)\n",
        "    goto = response[\"next\"]\n",
        "    if goto == \"FINISH\":\n",
        "        goto = END\n",
        "\n",
        "    return Command(goto=goto, update={\"next\": goto})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the additional specialized agents\n",
        "summarizer_agent = create_react_agent(\n",
        "    llm, tools=[summarize_text], prompt=\"You are a summarizer. Your job is to create concise summaries of text.\"\n",
        ")\n",
        "\n",
        "analyst_agent = create_react_agent(\n",
        "    llm, tools=[analyze_sentiment], prompt=\"You are an analyst. Your job is to analyze sentiment and provide insights.\"\n",
        ")\n",
        "\n",
        "\n",
        "def summarizer_node(state: State) -> Command[Literal[\"supervisor\"]]:\n",
        "    result = summarizer_agent.invoke(state)\n",
        "    return Command(\n",
        "        update={\n",
        "            \"messages\": [\n",
        "                HumanMessage(content=result[\"messages\"][-1].content, name=\"summarizer\")\n",
        "            ]\n",
        "        },\n",
        "        goto=\"supervisor\",\n",
        "    )\n",
        "\n",
        "\n",
        "def analyst_node(state: State) -> Command[Literal[\"supervisor\"]]:\n",
        "    result = analyst_agent.invoke(state)\n",
        "    return Command(\n",
        "        update={\n",
        "            \"messages\": [\n",
        "                HumanMessage(content=result[\"messages\"][-1].content, name=\"analyst\")\n",
        "            ]\n",
        "        },\n",
        "        goto=\"supervisor\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build the enhanced graph\n",
        "enhanced_builder = StateGraph(State)\n",
        "enhanced_builder.add_edge(START, \"supervisor\")\n",
        "enhanced_builder.add_node(\"supervisor\", enhanced_supervisor_node)\n",
        "enhanced_builder.add_node(\"researcher\", research_node)\n",
        "enhanced_builder.add_node(\"coder\", code_node)\n",
        "enhanced_builder.add_node(\"summarizer\", summarizer_node)\n",
        "enhanced_builder.add_node(\"analyst\", analyst_node)\n",
        "enhanced_graph = enhanced_builder.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the graph (uncomment if you have graphviz installed)\n",
        "# enhanced_graph.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the enhanced graph with a complex query\n",
        "complex_query = \"Research the latest climate change reports, summarize the main findings, analyze the sentiment of public responses, and create a visualization of temperature trends over the last decade.\"\n",
        "\n",
        "# Create an event stream\n",
        "events = enhanced_graph.stream({\"messages\": [{\"role\": \"user\", \"content\": complex_query}]})\n",
        "\n",
        "# Process each event\n",
        "for event in events:\n",
        "    node = event.get(\"node\")\n",
        "    if node is not None:\n",
        "        print(f\"\\n{'-' * 50}\")\n",
        "        print(f\"Currently executing node: {node}\")\n",
        "        if node == \"supervisor\":\n",
        "            print(f\"Next: {event['state'].get('next', 'Not specified')}\")\n",
        "    \n",
        "    state = event.get(\"state\")\n",
        "    if state is not None and \"messages\" in state:\n",
        "        # Print the last message\n",
        "        last_message = state[\"messages\"][-1]\n",
        "        if hasattr(last_message, \"name\") and last_message.name:\n",
        "            print(f\"\\n{last_message.name.upper()}: {last_message.content[:200]}...\")\n",
        "        else:\n",
        "            print(f\"\\n{last_message['role'].upper()}: {last_message['content'][:200]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating a Conditional Workflow\n",
        "\n",
        "Let's create a workflow with conditional branching based on user requirements:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Dict, List, Any\n",
        "\n",
        "# Define a function to route based on request classification\n",
        "def classify_request(state: State) -> Dict[str, Any]:\n",
        "    messages = state[\"messages\"]\n",
        "    user_message = [m for m in messages if m[\"role\"] == \"user\"][-1][\"content\"]\n",
        "    \n",
        "    # Use the LLM to classify the request\n",
        "    classification_prompt = f\"\"\"\n",
        "    Given the user request: \"{user_message}\"\n",
        "    \n",
        "    Classify this request into one of the following categories:\n",
        "    - RESEARCH: If the user is asking for information on a topic.\n",
        "    - ANALYSIS: If the user is asking for analysis of data or text.\n",
        "    - VISUALIZATION: If the user is asking for a chart or visualization.\n",
        "    - MULTI_TASK: If the user request includes multiple different types of tasks.\n",
        "    \n",
        "    Respond with just the category name.\n",
        "    \"\"\"\n",
        "    \n",
        "    classification_response = llm.invoke([\n",
        "        {\"role\": \"system\", \"content\": \"You are a classifier that categorizes user requests. Respond only with the category name.\"}, \n",
        "        {\"role\": \"user\", \"content\": classification_prompt}\n",
        "    ])\n",
        "    \n",
        "    category = classification_response.content.strip()\n",
        "    \n",
        "    # Return this as debug information, but it won't be added to state\n",
        "    return {\"classification\": category}\n",
        "\n",
        "\n",
        "def conditional_router(state: State, classification_result: Dict[str, Any]) -> str:\n",
        "    category = classification_result[\"classification\"]\n",
        "    \n",
        "    if \"RESEARCH\" in category:\n",
        "        return \"researcher\"\n",
        "    elif \"ANALYSIS\" in category:\n",
        "        return \"analyst\"\n",
        "    elif \"VISUALIZATION\" in category:\n",
        "        return \"coder\"\n",
        "    else:  # MULTI_TASK or anything else\n",
        "        return \"supervisor\"\n",
        "        \n",
        "# Build a conditional graph\n",
        "conditional_builder = StateGraph(State)\n",
        "conditional_builder.add_node(\"classifier\", classify_request)\n",
        "conditional_builder.add_node(\"supervisor\", enhanced_supervisor_node)\n",
        "conditional_builder.add_node(\"researcher\", research_node)\n",
        "conditional_builder.add_node(\"coder\", code_node)\n",
        "conditional_builder.add_node(\"summarizer\", summarizer_node)\n",
        "conditional_builder.add_node(\"analyst\", analyst_node)\n",
        "\n",
        "# Add conditional branching\n",
        "conditional_builder.add_edge(START, \"classifier\")\n",
        "conditional_builder.add_conditional_edges(\n",
        "    \"classifier\",\n",
        "    conditional_router,\n",
        "    {\n",
        "        \"researcher\": \"researcher\",\n",
        "        \"analyst\": \"analyst\",\n",
        "        \"coder\": \"coder\",\n",
        "        \"supervisor\": \"supervisor\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# Add remaining edges\n",
        "for member in enhanced_members:\n",
        "    conditional_builder.add_edge(member, \"supervisor\")\n",
        "\n",
        "# Compile the conditional graph\n",
        "conditional_graph = conditional_builder.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the conditional graph with different types of queries\n",
        "queries = [\n",
        "    \"What is the impact of climate change on polar bears?\",  # Research\n",
        "    \"Analyze the sentiment of this review: 'The hotel was excellent but the staff was rude.'\",  # Analysis\n",
        "    \"Create a bar chart showing the population of the 5 largest cities in the world.\",  # Visualization\n",
        "    \"Research renewable energy trends, summarize the findings, and create a visualization of adoption rates.\"  # Multi-task\n",
        "]\n",
        "\n",
        "for i, query in enumerate(queries):\n",
        "    print(f\"\\n\\n{'=' * 80}\\nQuery {i+1}: {query}\\n{'=' * 80}\")\n",
        "    \n",
        "    # Stream results\n",
        "    events = conditional_graph.stream({\"messages\": [{\"role\": \"user\", \"content\": query}]})\n",
        "    \n",
        "    # Process the first few events\n",
        "    event_count = 0\n",
        "    for event in events:\n",
        "        event_count += 1\n",
        "        node = event.get(\"node\")\n",
        "        if node is not None:\n",
        "            print(f\"\\n{'-' * 50}\")\n",
        "            print(f\"Currently executing node: {node}\")\n",
        "            if node == \"supervisor\":\n",
        "                print(f\"Next: {event['state'].get('next', 'Not specified')}\")\n",
        "        \n",
        "        state = event.get(\"state\")\n",
        "        if state is not None and \"messages\" in state:\n",
        "            # Print the last message\n",
        "            last_message = state[\"messages\"][-1]\n",
        "            if hasattr(last_message, \"name\") and last_message.name:\n",
        "                print(f\"\\n{last_message.name.upper()}: {last_message.content[:200]}...\")\n",
        "            elif isinstance(last_message, dict) and \"role\" in last_message:\n",
        "                print(f\"\\n{last_message['role'].upper()}: {last_message['content'][:200]}...\")\n",
        "                \n",
        "        # Limit the number of events we process for this demo\n",
        "        if event_count >= 5:\n",
        "            print(\"\\n... (Output truncated for brevity) ...\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, we've explored how to build complex agent workflows using LangGraph:\n",
        "\n",
        "1. **Basic Multi-Agent System**: A system with a supervisor that routes tasks to specialized agents (research and coding)\n",
        "2. **Enhanced Multi-Agent System**: Added more specialized agents for summarization and analysis\n",
        "3. **Conditional Workflows**: Created a system that classifies user queries and routes them to the appropriate agent first\n",
        "\n",
        "These patterns can be combined and extended to create powerful AI applications that can handle complex, multi-step tasks efficiently."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
} 